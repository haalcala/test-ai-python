{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690a8ba3-659a-486a-a46b-d94ca146801d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/haalcala/test-ai-python/blob/master/hugging-face/BERT.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941c59a8-af72-48f3-9380-2bb5b01c67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Collecting dataset\n",
      "  Downloading dataset-1.6.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.2 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from dataset) (1.4.7)\n",
      "Collecting banal>=1.0.1\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Collecting alembic>=0.6.2\n",
      "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\harol\\anaconda3\\lib\\site-packages (from alembic>=0.6.2->dataset) (3.10.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\harol\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\harol\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from importlib-metadata->alembic>=0.6.2->dataset) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from Mako->alembic>=0.6.2->dataset) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harol\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Installing collected packages: Mako, importlib-resources, tokenizers, huggingface-hub, banal, alembic, transformers, dataset\n",
      "Successfully installed Mako-1.2.4 alembic-1.9.4 banal-1.0.6 dataset-1.6.0 huggingface-hub-0.12.1 importlib-resources-5.12.0 tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a029ad-96b1-4a86-b6c1-88ace25595eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab10277-7419-4c82-aca7-8fd3a646405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9850340485572815},\n",
       " {'label': 'NEGATIVE', 'score': 0.8457930684089661}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\"I've been waiting for HuggingFace course my whole life.\", \"The quick brown fox jumps over the lazy dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f7670a-291d-4a11-9413-0e9daa0a1889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6a971ea7e44184bd54fea4986de2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f23feb591c4b04bd1e9a22241117fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86cc6f580fc413a876e12bbc932aee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4f685050d647e19d42f17b8a26c276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c16e440eae49a3a2ceda12e2bb45cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35407714daa441439b9f86203c797ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'The quick brown fox jumps over the lazy dog',\n",
       "  'labels': ['code',\n",
       "   'zoo',\n",
       "   'business',\n",
       "   'spam',\n",
       "   'education',\n",
       "   'garbage',\n",
       "   'politics'],\n",
       "  'scores': [0.2797161638736725,\n",
       "   0.25859448313713074,\n",
       "   0.25182563066482544,\n",
       "   0.0620739720761776,\n",
       "   0.05405884236097336,\n",
       "   0.052496593445539474,\n",
       "   0.041234325617551804]},\n",
       " {'sequence': 'from sklearn.datasets import make_multilabel_classification',\n",
       "  'labels': ['code',\n",
       "   'zoo',\n",
       "   'education',\n",
       "   'business',\n",
       "   'politics',\n",
       "   'spam',\n",
       "   'garbage'],\n",
       "  'scores': [0.6581407189369202,\n",
       "   0.10895588994026184,\n",
       "   0.10061231255531311,\n",
       "   0.07643519341945648,\n",
       "   0.028996406123042107,\n",
       "   0.015686584636569023,\n",
       "   0.011172881349921227]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "classifier([\"The quick brown fox jumps over the lazy dog\", \"from sklearn.datasets import make_multilabel_classification\"],\n",
    "          candidate_labels=[\"garbage\", \"spam\", \"education\", \"politics\", \"business\", \"zoo\", \"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d9ead7-0a6c-40d6-9b51-dcd92c3b964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe564c392324ebdac85123892a66efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\harol\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The quick brown fox jumps over the fire to protect the fire, but it takes several rounds. The fox then quickly tries to attack but the Fox breaks free and runs at the fox and punches it in the face. The fox then punches back at the'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\", max_length=50)\n",
    "# generator = pipeline(\"text-generation\", model=\"gpt2-xl\", max_length=50)\n",
    "# generator = pipeline(\"text-generation\", model=\"bert-base-cased\", max_length=50)\n",
    "\n",
    "generator(\"The quick brown fox jumps over the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eec5453-d124-4f87-8b39-a4060a84d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:629: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6192b79af3b7420fad89af5992ed33f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)learn_model.joblib\";:   0%|          | 0.00/187k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator Pipeline from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_url, cached_download\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "REPO_ID = \"julien-c/wine-quality\"\n",
    "FILENAME = \"sklearn_model.joblib\"\n",
    "\n",
    "\n",
    "model = joblib.load(cached_download(\n",
    "    hf_hub_url(REPO_ID, FILENAME)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb949fab-2ab5-4b89-bf2b-68f880f7001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b34720a860c4f0faef6c26e7e4e6dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/winequality-red.csv:   0%|          | 0.00/84.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "\n",
      "   alcohol  \n",
      "0      9.4  \n",
      "1      9.8  \n",
      "2      9.8  \n"
     ]
    }
   ],
   "source": [
    "data_file = cached_download(\n",
    "    hf_hub_url(REPO_ID, \"winequality-red.csv\")\n",
    ")\n",
    "winedf = pd.read_csv(data_file, sep=\";\")\n",
    "\n",
    "\n",
    "X = winedf.drop([\"quality\"], axis=1)\n",
    "Y = winedf[\"quality\"]\n",
    "\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87ca95fc-0054-431a-bf10-2f8e9494bb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = model.predict(X[:3])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a38a98f-8b6f-4ef8-b759-067043714dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6616635397123202"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9230b38-ace8-4986-a7df-7d54564610ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp38-cp38-win_amd64.whl (162.6 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\harol\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7583dd-bb6d-49bc-a5c3-d2610971410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: tell me a joke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: What's the difference between a joke and a joke?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: is that the joke?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Is that the joke?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: joker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Is that the joke?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: nah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Is that the joke?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Is that the joke?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a414ae-6369-4ce8-8af8-790308e28bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a597e0a29c5a49e7be22f08e352ed2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0260e3eef54be9aa4e3211f6abf591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e8f90c49194edbb9a20112fd8a3102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7b83625a6e4772b015ee5bc61da9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e1b38be2104ac59d1984071cd40cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09019211679697037,\n",
       "  'token': 4633,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"Hello I'm a fashion model.\"},\n",
       " {'score': 0.06349997222423553,\n",
       "  'token': 1207,\n",
       "  'token_str': 'new',\n",
       "  'sequence': \"Hello I'm a new model.\"},\n",
       " {'score': 0.06228204444050789,\n",
       "  'token': 2581,\n",
       "  'token_str': 'male',\n",
       "  'sequence': \"Hello I'm a male model.\"},\n",
       " {'score': 0.04417279735207558,\n",
       "  'token': 1848,\n",
       "  'token_str': 'professional',\n",
       "  'sequence': \"Hello I'm a professional model.\"},\n",
       " {'score': 0.03326147049665451,\n",
       "  'token': 7688,\n",
       "  'token_str': 'super',\n",
       "  'sequence': \"Hello I'm a super model.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7510df7-1a03-48a9-ac6f-d34782a761b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6596803665161133,\n",
       "  'token': 138,\n",
       "  'token_str': 'A',\n",
       "  'sequence': 'A quick brown fox jumps over the lazy dog.'},\n",
       " {'score': 0.28083592653274536,\n",
       "  'token': 1109,\n",
       "  'token_str': 'The',\n",
       "  'sequence': 'The quick brown fox jumps over the lazy dog.'},\n",
       " {'score': 0.013413644395768642,\n",
       "  'token': 2543,\n",
       "  'token_str': 'Another',\n",
       "  'sequence': 'Another quick brown fox jumps over the lazy dog.'},\n",
       " {'score': 0.011517303064465523,\n",
       "  'token': 1448,\n",
       "  'token_str': 'One',\n",
       "  'sequence': 'One quick brown fox jumps over the lazy dog.'},\n",
       " {'score': 0.007986655458807945,\n",
       "  'token': 170,\n",
       "  'token_str': 'a',\n",
       "  'sequence': 'a quick brown fox jumps over the lazy dog.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\" [MASK] quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d036bf-e02e-4203-8eee-97eb0cc775b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bc9aa8-5346-4e7a-8f2e-43ff5a5c3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  My friends are cool but they eat too many carbs.\n",
      "Bot:   That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?\n",
      "Human:  I'm not sure\n",
      "Bot:   I see. Well, it's good that they're trying to change their eating habits.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "print(\"Human: \", UTTERANCE)\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "REPLY = \"I'm not sure\"\n",
    "print(\"Human: \", REPLY)\n",
    "\n",
    "NEXT_UTTERANCE = (\n",
    "    \"My friends are cool but they eat too many carbs.</s> <s>That's unfortunate. \"\n",
    "    \"Are they trying to lose weight or are they just trying to be healthier?</s> \"\n",
    "    \"<s> I'm not sure.\"\n",
    ")\n",
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n",
    "next_reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9026c087-658c-43bf-8b92-e2d546166895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd9783e1009487688cb533b327809a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971cbbf306b24a75b38b721e97dd463e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/797M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5acb40d0fbf4b4bb13b751145df912e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/240 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7cce48a9ae49c586b7fe1fd8f9ad93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea0f37b30d541fab64155fb7df9f022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38eac709f5704a65b25c4ba1faaf4c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575bdf9773fd4ca7991160f3623fae5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed6f7649c4e4e73b78d2cfa19852752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\transformers\\models\\codegen\\modeling_codegen.py:166: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world():\n",
      "    print(\"Hello World\")\n",
      "\n",
      "hello_world()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "text = \"def hello_world():\"\n",
    "\n",
    "completion = model.generate(**tokenizer(text, return_tensors=\"pt\"))\n",
    "\n",
    "print(tokenizer.decode(completion[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3de8ca1-0db5-4b84-a037-a1454c9b788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I\\'m going to sleep tonight. The idea that we have to do this together has nothing to do with the relationship between the people we love that we have. And that\\'s what we\\'ve got to do.\"\\n\\n\\n\\n\\n\\n“'},\n",
       " {'generated_text': 'I\\'m going to sleep this, and I won\\'t take it for granted,\" he said.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\"I'm going to sleep\", max_length=50, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e5af94a-81d3-43f0-95a0-cc2f24341012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbac5c3f34d7413c9950a4b3a3094d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d53a4c38bbb450ab41768770ecb62a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeee712c73f4e1fbcba5850273ed964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae29d186aa074322abdc26ec5eadb6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc3d5c6251e4d6583b8130f42b4c5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0589599646627903,\n",
       "  'token': 2039,\n",
       "  'token_str': ' missed',\n",
       "  'sequence': 'This will be missed'},\n",
       " {'score': 0.052137341350317,\n",
       "  'token': 4752,\n",
       "  'token_str': ' updated',\n",
       "  'sequence': 'This will be updated'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This will be <mask>\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3be6744e-499f-441f-bbf1-9a2cc8743dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99413157,\n",
       "  'word': 'Harold Alcala',\n",
       "  'start': 4,\n",
       "  'end': 17},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.99126524,\n",
       "  'word': 'V - Cube',\n",
       "  'start': 33,\n",
       "  'end': 39},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9997681,\n",
       "  'word': 'Singapore',\n",
       "  'start': 43,\n",
       "  'end': 52}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "ner(\"I'm Harold Alcala and I work for V-Cube in Singapore. I am a Programmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe858544-d391-4507-acd1-850fa70472a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.30629605054855347,\n",
       " 'start': 561,\n",
       " 'end': 623,\n",
       " 'answer': 'improve the current situation through technological innovation'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "question_answerer(\n",
    "    context=\"\"\"\n",
    "“The more seeable, the more we see”\n",
    "\n",
    "Business and learning are born out of communication. That is why, meeting is essential for business and learning. Because when we meet, we are able to convey things that can neither be conveyed via phone called nor erased from letters and e-mails. We know this from experience.\n",
    "\n",
    "However, meetings are no long limited to a 20km radius. Nowadays, we need to travel distances that are only accessible by planes, trains and cars. Some destinations could take at least a day to reach. There is never enough time. Our goal is to improve the current situation through technological innovation.\n",
    "\n",
    "V-CUBE introduces a new dynamic to web conferencing with best technology, allowing communication to be as good as seeing all parties in person.Our goals are not to simply \"offer a tool that makes web conference more convenient\".We look to enable users to visualize communication beyond the limitations of phone calls, letters and emails.When we are able to “see” communication, we would be able to see all nuances to understand the underlying message beyond what words can convey.Relationship with coworkers and customers is improved without meeting in person. Cost for such purposes are reduced.\n",
    "\n",
    "Therefore our goal is to help users visualize information, cost and people more effectively.\n",
    "\n",
    "Through visible communication, V-CUBE aims to not only change the communication styles of businesses and learning environments in Japan, but everywhere else in the world.\n",
    "\n",
    "V-CUBE, “Seeing” Communication\n",
    "\"\"\",\n",
    "    question=\"What is the goal here?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1f4080f-1544-49d7-8224-801bb936cf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9a85b0f98745a6982e89dda5b22206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f972c08054ea47ff986d568b31243140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77f6af5ea6840a8ae28fd7b500245e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59121416eb74bcb8592f311fca7fbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e78cdf4a2f8452d92ec0db7156441bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' V-CUBE introduces a new dynamic to web conferencing with best technology, allowing communication to be as good as seeing all parties in person . The more seeable, the more we see, we would be able to see all nuances to understand the underlying message beyond words can convey .'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summariser = pipeline(\"summarization\")\n",
    "\n",
    "summariser(\"\"\"\n",
    "“The more seeable, the more we see”\n",
    "\n",
    "Business and learning are born out of communication. That is why, meeting is essential for business and learning. Because when we meet, we are able to convey things that can neither be conveyed via phone called nor erased from letters and e-mails. We know this from experience.\n",
    "\n",
    "However, meetings are no long limited to a 20km radius. Nowadays, we need to travel distances that are only accessible by planes, trains and cars. Some destinations could take at least a day to reach. There is never enough time. Our goal is to improve the current situation through technological innovation.\n",
    "\n",
    "V-CUBE introduces a new dynamic to web conferencing with best technology, allowing communication to be as good as seeing all parties in person.Our goals are not to simply \"offer a tool that makes web conference more convenient\".We look to enable users to visualize communication beyond the limitations of phone calls, letters and emails.When we are able to “see” communication, we would be able to see all nuances to understand the underlying message beyond what words can convey.Relationship with coworkers and customers is improved without meeting in person. Cost for such purposes are reduced.\n",
    "\n",
    "Therefore our goal is to help users visualize information, cost and people more effectively.\n",
    "\n",
    "Through visible communication, V-CUBE aims to not only change the communication styles of businesses and learning environments in Japan, but everywhere else in the world.\n",
    "\n",
    "V-CUBE, “Seeing” Communication\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbd8d5-6d2a-4621-9fc7-e61b4bd36f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
