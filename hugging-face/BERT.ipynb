{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690a8ba3-659a-486a-a46b-d94ca146801d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/haalcala/test-ai-python/blob/master/hugging-face/BERT.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1c8732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter_console\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter_console) (3.0.38)\n",
      "Requirement already satisfied: ipykernel>=6.14 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter_console) (6.15.0)\n",
      "Requirement already satisfied: traitlets>=5.4 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter_console) (5.9.0)\n",
      "Requirement already satisfied: ipython in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter_console) (8.11.0)\n",
      "Requirement already satisfied: pygments in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter_console) (2.14.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter_console) (19.0.2)\n",
      "Requirement already satisfied: jupyter-client>=7.0.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter_console) (7.0.6)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter_console) (5.2.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipykernel>=6.14->jupyter_console) (1.5.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipykernel>=6.14->jupyter_console) (6.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipykernel>=6.14->jupyter_console) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipykernel>=6.14->jupyter_console) (1.5.6)\n",
      "Requirement already satisfied: psutil in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipykernel>=6.14->jupyter_console) (5.9.0)\n",
      "Requirement already satisfied: packaging in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipykernel>=6.14->jupyter_console) (23.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipython->jupyter_console) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipython->jupyter_console) (0.6.2)\n",
      "Requirement already satisfied: backcall in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipython->jupyter_console) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipython->jupyter_console) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipython->jupyter_console) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from ipython->jupyter_console) (0.7.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter-client>=7.0.0->jupyter_console) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter-client>=7.0.0->jupyter_console) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter_console) (3.1.0)\n",
      "Requirement already satisfied: wcwidth in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from prompt-toolkit>=3.0.30->jupyter_console) (0.2.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from jedi>=0.16->ipython->jupyter_console) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from pexpect>4.3->ipython->jupyter_console) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client>=7.0.0->jupyter_console) (1.16.0)\n",
      "Requirement already satisfied: pure-eval in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from stack-data->ipython->jupyter_console) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from stack-data->ipython->jupyter_console) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from stack-data->ipython->jupyter_console) (2.2.1)\n",
      "Installing collected packages: jupyter_console\n",
      "Successfully installed jupyter_console-6.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U jupyter_console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff118fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  9 06:46:20 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.63       Driver Version: 512.36       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   45C    P8    16W /  N/A |    809MiB / 16384MiB |     38%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     23849      G   /Xwayland                       N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cede7640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Collecting tensorflow==2.11.*\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch==1.11.0+cu113 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision==0.12.0+cu113 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: torchaudio==0.11.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: packaging in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.11.*) (23.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.11.*) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.11.*) (4.5.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Using cached libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.11.*) (1.24.2)\n",
      "Requirement already satisfied: setuptools in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow==2.11.*) (65.6.3)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from torchvision==0.12.0+cu113) (9.4.0)\n",
      "Requirement already satisfied: requests in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from torchvision==0.12.0+cu113) (2.28.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.11.*) (0.38.4)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->torchvision==0.12.0+cu113) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->torchvision==0.12.0+cu113) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->torchvision==0.12.0+cu113) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->torchvision==0.12.0+cu113) (3.1.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, pyasn1, libclang, flatbuffers, zipp, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, MarkupSafe, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, werkzeug, requests-oauthlib, importlib-metadata, google-auth, markdown, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 h5py-3.8.0 importlib-metadata-6.0.0 keras-2.11.0 libclang-15.0.6.1 markdown-3.4.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 werkzeug-2.2.3 wrapt-1.15.0 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.11.* torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e928a413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 06:46:31.891546: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 06:46:32.099969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:32.099987: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 06:46:32.857030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:32.857092: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:32.857097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 06:46:33.805848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-09 06:46:33.808253: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:33.808307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:33.808332: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:33.808356: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:33.867260: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:33.867308: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-09 06:46:33.867322: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313f437e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5bbffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "941c59a8-af72-48f3-9380-2bb5b01c67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.8/661.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.0/770.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: requests in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (1.24.2)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=17.3.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, pytz, xxhash, tqdm, regex, pyyaml, pyarrow, multidict, fsspec, frozenlist, filelock, dill, attrs, async-timeout, yarl, responses, pandas, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-22.2.0 datasets-2.10.1 dill-0.3.6 filelock-3.9.0 frozenlist-1.3.3 fsspec-2023.3.0 huggingface-hub-0.13.0 multidict-6.0.4 multiprocess-0.70.14 pandas-1.5.3 pyarrow-11.0.0 pytz-2022.7.1 pyyaml-6.0 regex-2022.10.31 responses-0.18.0 tokenizers-0.13.2 tqdm-4.65.0 transformers-4.26.1 xxhash-3.2.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a029ad-96b1-4a86-b6c1-88ace25595eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab10277-7419-4c82-aca7-8fd3a646405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9850157499313354},\n",
       " {'label': 'NEGATIVE', 'score': 0.8461747765541077}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", device=device)\n",
    "classifier([\"I've been waiting for HuggingFace course my whole life.\", \"The quick brown fox jumps over the lazy dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7f8fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a690ed73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"finetuning_task\": \"sst-2\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f7670a-291d-4a11-9413-0e9daa0a1889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.15k/1.15k [00:00<00:00, 390kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [01:24<00:00, 19.4MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 7.61kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:02<00:00, 386kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:01<00:00, 331kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:03<00:00, 398kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'The quick brown fox jumps over the lazy dog',\n",
       "  'labels': ['code',\n",
       "   'zoo',\n",
       "   'business',\n",
       "   'spam',\n",
       "   'education',\n",
       "   'garbage',\n",
       "   'politics'],\n",
       "  'scores': [0.27989810705184937,\n",
       "   0.2582353949546814,\n",
       "   0.2521042823791504,\n",
       "   0.062041353434324265,\n",
       "   0.05399523302912712,\n",
       "   0.052396856248378754,\n",
       "   0.04132877662777901]},\n",
       " {'sequence': 'from sklearn.datasets import make_multilabel_classification',\n",
       "  'labels': ['code',\n",
       "   'zoo',\n",
       "   'education',\n",
       "   'business',\n",
       "   'politics',\n",
       "   'spam',\n",
       "   'garbage'],\n",
       "  'scores': [0.6589826941490173,\n",
       "   0.10862106829881668,\n",
       "   0.10032610595226288,\n",
       "   0.07626305520534515,\n",
       "   0.02897052839398384,\n",
       "   0.01566215418279171,\n",
       "   0.011174396611750126]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", device=device)\n",
    "\n",
    "classifier([\"The quick brown fox jumps over the lazy dog\", \"from sklearn.datasets import make_multilabel_classification\"],\n",
    "          candidate_labels=[\"garbage\", \"spam\", \"education\", \"politics\", \"business\", \"zoo\", \"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37f70d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"facebook/bart-large-mnli\",\n",
       "  \"_num_labels\": 3,\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"contradiction\",\n",
       "    \"1\": \"neutral\",\n",
       "    \"2\": \"entailment\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"contradiction\": 0,\n",
       "    \"entailment\": 2,\n",
       "    \"neutral\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"normalize_before\": false,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": false,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": false,\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d9ead7-0a6c-40d6-9b51-dcd92c3b964c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 419kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 548M/548M [00:31<00:00, 17.4MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 71.0kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 414kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:01<00:00, 333kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:02<00:00, 485kB/s]\n",
      "/home/harold/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The quick brown fox jumps over the back of the car and tries to attack its pursuers.\\n\\n\"The last one. I was trying to shoot them, and I hit the one with my gun.\" Dr. Strang\\n\\nSick'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\", max_length=50)\n",
    "# generator = pipeline(\"text-generation\", model=\"gpt2-xl\", max_length=50)\n",
    "# generator = pipeline(\"text-generation\", model=\"bert-base-cased\", max_length=50)\n",
    "\n",
    "generator(\"The quick brown fox jumps over the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8f13ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"do_sample\": true,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"max_length\": 50,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05740a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: pandas in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/harold/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib pandas numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eec5453-d124-4f87-8b39-a4060a84d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harold/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/harold/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/harold/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator Pipeline from version 0.24.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_url, cached_download\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "REPO_ID = \"julien-c/wine-quality\"\n",
    "FILENAME = \"sklearn_model.joblib\"\n",
    "\n",
    "\n",
    "model = joblib.load(cached_download(\n",
    "    hf_hub_url(REPO_ID, FILENAME)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb949fab-2ab5-4b89-bf2b-68f880f7001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harold/miniconda3/envs/tf/lib/python3.9/site-packages/huggingface_hub/file_download.py:637: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "Downloading (…)/winequality-red.csv: 100%|██████████| 84.2k/84.2k [00:00<00:00, 183kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "\n",
      "   alcohol  \n",
      "0      9.4  \n",
      "1      9.8  \n",
      "2      9.8  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = cached_download(\n",
    "    hf_hub_url(REPO_ID, \"winequality-red.csv\")\n",
    ")\n",
    "winedf = pd.read_csv(data_file, sep=\";\")\n",
    "\n",
    "\n",
    "X = winedf.drop([\"quality\"], axis=1)\n",
    "Y = winedf[\"quality\"]\n",
    "\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ca95fc-0054-431a-bf10-2f8e9494bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harold/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = model.predict(X[:3])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a38a98f-8b6f-4ef8-b759-067043714dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harold/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6616635397123202"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9230b38-ace8-4986-a7df-7d54564610ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp38-cp38-win_amd64.whl (162.6 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\harol\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de7583dd-bb6d-49bc-a5c3-d2610971410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 07:07:19.464811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 07:07:19.631903: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 07:07:19.631943: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 07:07:20.320052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 07:07:20.320133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 07:07:20.320138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 8.20kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 642/642 [00:00<00:00, 271kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 644kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 500kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.75G/1.75G [01:26<00:00, 20.2MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 14.7kB/s]\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Cloudy and raining.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: About a billion lightyears.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: 1.5 billion lightyears.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm not sure what you mean by this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I think he means the quick brown fox jumps over the moon.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/DialoGPT-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a414ae-6369-4ce8-8af8-790308e28bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 319kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 436M/436M [00:26<00:00, 16.2MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 15.6kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 312kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:02<00:00, 183kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09019213169813156,\n",
       "  'token': 4633,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"Hello I'm a fashion model.\"},\n",
       " {'score': 0.06349997967481613,\n",
       "  'token': 1207,\n",
       "  'token_str': 'new',\n",
       "  'sequence': \"Hello I'm a new model.\"},\n",
       " {'score': 0.06228199601173401,\n",
       "  'token': 2581,\n",
       "  'token_str': 'male',\n",
       "  'sequence': \"Hello I'm a male model.\"},\n",
       " {'score': 0.04417289048433304,\n",
       "  'token': 1848,\n",
       "  'token_str': 'professional',\n",
       "  'sequence': \"Hello I'm a professional model.\"},\n",
       " {'score': 0.03326144441962242,\n",
       "  'token': 7688,\n",
       "  'token_str': 'super',\n",
       "  'sequence': \"Hello I'm a super model.\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7510df7-1a03-48a9-ac6f-d34782a761b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6596804857254028,\n",
       "  'token': 138,\n",
       "  'token_str': 'A',\n",
       "  'sequence': 'A quick brown fox jumps over the lazy dog.'},\n",
       " {'score': 0.28083568811416626,\n",
       "  'token': 1109,\n",
       "  'token_str': 'The',\n",
       "  'sequence': 'The quick brown fox jumps over the lazy dog.'},\n",
       " {'score': 0.013413634151220322,\n",
       "  'token': 2543,\n",
       "  'token_str': 'Another',\n",
       "  'sequence': 'Another quick brown fox jumps over the lazy dog.'},\n",
       " {'score': 0.011517282575368881,\n",
       "  'token': 1448,\n",
       "  'token_str': 'One',\n",
       "  'sequence': 'One quick brown fox jumps over the lazy dog.'},\n",
       " {'score': 0.007986649870872498,\n",
       "  'token': 170,\n",
       "  'token_str': 'a',\n",
       "  'sequence': 'a quick brown fox jumps over the lazy dog.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\" [MASK] quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c3ece6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d036bf-e02e-4203-8eee-97eb0cc775b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bc9aa8-5346-4e7a-8f2e-43ff5a5c3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  My friends are cool but they eat too many carbs.\n",
      "Bot:   That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?\n",
      "Human:  I'm not sure\n",
      "Bot:   I see. Well, it's good that they're trying to change their eating habits.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "print(\"Human: \", UTTERANCE)\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "REPLY = \"I'm not sure\"\n",
    "print(\"Human: \", REPLY)\n",
    "\n",
    "NEXT_UTTERANCE = (\n",
    "    \"My friends are cool but they eat too many carbs.</s> <s>That's unfortunate. \"\n",
    "    \"Are they trying to lose weight or are they just trying to be healthier?</s> \"\n",
    "    \"<s> I'm not sure.\"\n",
    ")\n",
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n",
    "next_reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9026c087-658c-43bf-8b92-e2d546166895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd9783e1009487688cb533b327809a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971cbbf306b24a75b38b721e97dd463e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/797M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5acb40d0fbf4b4bb13b751145df912e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/240 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7cce48a9ae49c586b7fe1fd8f9ad93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea0f37b30d541fab64155fb7df9f022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38eac709f5704a65b25c4ba1faaf4c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575bdf9773fd4ca7991160f3623fae5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed6f7649c4e4e73b78d2cfa19852752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\transformers\\models\\codegen\\modeling_codegen.py:166: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world():\n",
      "    print(\"Hello World\")\n",
      "\n",
      "hello_world()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "text = \"def hello_world():\"\n",
    "\n",
    "completion = model.generate(**tokenizer(text, return_tensors=\"pt\"))\n",
    "\n",
    "print(tokenizer.decode(completion[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3de8ca1-0db5-4b84-a037-a1454c9b788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I\\'m going to sleep tonight. The idea that we have to do this together has nothing to do with the relationship between the people we love that we have. And that\\'s what we\\'ve got to do.\"\\n\\n\\n\\n\\n\\n“'},\n",
       " {'generated_text': 'I\\'m going to sleep this, and I won\\'t take it for granted,\" he said.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\"I'm going to sleep\", max_length=50, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e5af94a-81d3-43f0-95a0-cc2f24341012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbac5c3f34d7413c9950a4b3a3094d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d53a4c38bbb450ab41768770ecb62a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeee712c73f4e1fbcba5850273ed964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae29d186aa074322abdc26ec5eadb6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc3d5c6251e4d6583b8130f42b4c5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0589599646627903,\n",
       "  'token': 2039,\n",
       "  'token_str': ' missed',\n",
       "  'sequence': 'This will be missed'},\n",
       " {'score': 0.052137341350317,\n",
       "  'token': 4752,\n",
       "  'token_str': ' updated',\n",
       "  'sequence': 'This will be updated'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This will be <mask>\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3be6744e-499f-441f-bbf1-9a2cc8743dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99413157,\n",
       "  'word': 'Harold Alcala',\n",
       "  'start': 4,\n",
       "  'end': 17},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.99126524,\n",
       "  'word': 'V - Cube',\n",
       "  'start': 33,\n",
       "  'end': 39},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9997681,\n",
       "  'word': 'Singapore',\n",
       "  'start': 43,\n",
       "  'end': 52}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "ner(\"I'm Harold Alcala and I work for V-Cube in Singapore. I am a Programmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe858544-d391-4507-acd1-850fa70472a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.30629605054855347,\n",
       " 'start': 561,\n",
       " 'end': 623,\n",
       " 'answer': 'improve the current situation through technological innovation'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "question_answerer(\n",
    "    context=\"\"\"\n",
    "“The more seeable, the more we see”\n",
    "\n",
    "Business and learning are born out of communication. That is why, meeting is essential for business and learning. Because when we meet, we are able to convey things that can neither be conveyed via phone called nor erased from letters and e-mails. We know this from experience.\n",
    "\n",
    "However, meetings are no long limited to a 20km radius. Nowadays, we need to travel distances that are only accessible by planes, trains and cars. Some destinations could take at least a day to reach. There is never enough time. Our goal is to improve the current situation through technological innovation.\n",
    "\n",
    "V-CUBE introduces a new dynamic to web conferencing with best technology, allowing communication to be as good as seeing all parties in person.Our goals are not to simply \"offer a tool that makes web conference more convenient\".We look to enable users to visualize communication beyond the limitations of phone calls, letters and emails.When we are able to “see” communication, we would be able to see all nuances to understand the underlying message beyond what words can convey.Relationship with coworkers and customers is improved without meeting in person. Cost for such purposes are reduced.\n",
    "\n",
    "Therefore our goal is to help users visualize information, cost and people more effectively.\n",
    "\n",
    "Through visible communication, V-CUBE aims to not only change the communication styles of businesses and learning environments in Japan, but everywhere else in the world.\n",
    "\n",
    "V-CUBE, “Seeing” Communication\n",
    "\"\"\",\n",
    "    question=\"What is the goal here?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1f4080f-1544-49d7-8224-801bb936cf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9a85b0f98745a6982e89dda5b22206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f972c08054ea47ff986d568b31243140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77f6af5ea6840a8ae28fd7b500245e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59121416eb74bcb8592f311fca7fbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e78cdf4a2f8452d92ec0db7156441bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' V-CUBE introduces a new dynamic to web conferencing with best technology, allowing communication to be as good as seeing all parties in person . The more seeable, the more we see, we would be able to see all nuances to understand the underlying message beyond words can convey .'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summariser = pipeline(\"summarization\")\n",
    "\n",
    "summariser(\"\"\"\n",
    "“The more seeable, the more we see”\n",
    "\n",
    "Business and learning are born out of communication. That is why, meeting is essential for business and learning. Because when we meet, we are able to convey things that can neither be conveyed via phone called nor erased from letters and e-mails. We know this from experience.\n",
    "\n",
    "However, meetings are no long limited to a 20km radius. Nowadays, we need to travel distances that are only accessible by planes, trains and cars. Some destinations could take at least a day to reach. There is never enough time. Our goal is to improve the current situation through technological innovation.\n",
    "\n",
    "V-CUBE introduces a new dynamic to web conferencing with best technology, allowing communication to be as good as seeing all parties in person.Our goals are not to simply \"offer a tool that makes web conference more convenient\".We look to enable users to visualize communication beyond the limitations of phone calls, letters and emails.When we are able to “see” communication, we would be able to see all nuances to understand the underlying message beyond what words can convey.Relationship with coworkers and customers is improved without meeting in person. Cost for such purposes are reduced.\n",
    "\n",
    "Therefore our goal is to help users visualize information, cost and people more effectively.\n",
    "\n",
    "Through visible communication, V-CUBE aims to not only change the communication styles of businesses and learning environments in Japan, but everywhere else in the world.\n",
    "\n",
    "V-CUBE, “Seeing” Communication\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbd8d5-6d2a-4621-9fc7-e61b4bd36f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
